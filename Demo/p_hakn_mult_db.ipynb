{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent folder path where lib folder is\n",
    "import sys\n",
    "if \"..\" not in sys.path:import sys; sys.path.insert(0, '..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonparametric tests for one performance measure (e.g., AUC)\n",
    "from P_HAK import run_friedman, stac_multi, ap2h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.05   # Set this to change the default signifance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the csv, the columns are classifiers and the rows are datasets \n",
    "full_df = pd.read_csv(\"../datasets/gmeans.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_vs_One: list of \"control\" classifiers\n",
    "#avo = []\n",
    "avo = ['RF']\n",
    "#avo = ['RF','XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(avo) == 0:\n",
    "    oname = \"All_Models\"\n",
    "    df = full_df\n",
    "else:\n",
    "    baseclf = tuple(avo)\n",
    "    oname=\"\"\n",
    "    for x in range(len(avo)):\n",
    "        oname += avo[x] + \"_\"\n",
    "    oname += \"Models\"\n",
    "    df = full_df.loc[:, full_df.columns.str.startswith(baseclf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Friedman test \n",
    "Checks if there is a significant difference in performance for any classifier<br>\n",
    "If we reject H0 (no difference), we use the post-hoc test to find out which differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, rptstr, rankings = run_friedman(df)\n",
    "print(oname,\":\",rptstr)\n",
    "\n",
    "# continue only if H0 was rejected\n",
    "if not reject:\n",
    "    raise Exception(\"Accepted H0 for Freidman Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### All vs. All Tests\n",
    "Compare every classifier to every other one, using the rankings ('pivotal values')  returned from the Freidman test.<br>\n",
    "General case shows p_values adjusted for multiple tests using a range of methods, Nemenyi and Shaffer show p_values adjusted using similar methods.Note that for technical reasons, the Schaffer method should not be used for more than 18 classifiers.<br>\n",
    "The dataframe of adjusted p_values can be quickly converted to show if the Null Hypothesis (H0: No significant difference) should be accepted (True) or rejected (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pvals_df = stac_multi(rankings)\n",
    "gen_pvals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ho_df = ap2h0(gen_pvals_df)\n",
    "gen_ho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nym_ap_df=stac_multi(rankings,nmyi=True)\n",
    "nym_ap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One vs All (Control vs Treatment) \n",
    "In some cases, we do not care about all pairwise comparisons as we only propose a single method, or just need to compare to a baseline method. In this case we designate a control method, and compare all others to it.<br>\n",
    "For statistical reasons the Nemenyi test should not be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ap_df = stac_multi(rankings,control='RF')\n",
    "xgb_ap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ho_df = ap2h0(xgb_ap_df)\n",
    "xgb_ho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaf_ap_xgb_df = stac_multi(rankings,shaf=True,control='RF')\n",
    "shaf_ap_xgb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
